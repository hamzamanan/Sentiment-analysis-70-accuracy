{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Hamza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hamza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Hamza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "positive_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tokens = twitter_samples.tokenized('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@DespiteOfficial', 'JJ'),\n",
       " ('we', 'PRP'),\n",
       " ('had', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('listen', 'VBN'),\n",
       " ('last', 'JJ'),\n",
       " ('night', 'NN'),\n",
       " (':)', 'NN'),\n",
       " ('As', 'IN'),\n",
       " ('You', 'PRP'),\n",
       " ('Bleed', 'VBP'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('amazing', 'JJ'),\n",
       " ('track', 'NN'),\n",
       " ('.', '.'),\n",
       " ('When', 'WRB'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('in', 'IN'),\n",
       " ('Scotland', 'NNP'),\n",
       " ('?', '.'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parts of speech\n",
    "from nltk.tag import pos_tag\n",
    "position_tag = pos_tag(positive_tokens[2])\n",
    "position_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@DespiteOfficial',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " 'listen',\n",
       " 'last',\n",
       " 'night',\n",
       " ':)',\n",
       " 'As',\n",
       " 'You',\n",
       " 'Bleed',\n",
       " 'be',\n",
       " 'an',\n",
       " 'amaze',\n",
       " 'track',\n",
       " '.',\n",
       " 'When',\n",
       " 'be',\n",
       " 'you',\n",
       " 'in',\n",
       " 'Scotland',\n",
       " '?',\n",
       " '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma = []\n",
    "for word, tag in position_tag:\n",
    "    lemma.append(lemmatizer.lemmatize(word, pos = 'v'))\n",
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hamza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@DespiteOfficial',\n",
       " 'listen',\n",
       " 'last',\n",
       " 'night',\n",
       " ':)',\n",
       " 'As',\n",
       " 'You',\n",
       " 'Bleed',\n",
       " 'amaze',\n",
       " 'track',\n",
       " '.',\n",
       " 'When',\n",
       " 'Scotland',\n",
       " '?',\n",
       " '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_stop = [i for i in lemma if i not in stop_words]\n",
    "without_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_list = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        token = token.lower()\n",
    "        token = re.sub(r'@[a-z0-9_]\\S+', '', token)\n",
    "        token = re.sub(r'#[a-z0-9_]\\S+', '', token)\n",
    "        token = re.sub(r'&[a-z0-9_]\\S+', '', token)\n",
    "        token = re.sub(r'[?!.+,;$£%&\"]+', '', token)\n",
    "        token = re.sub(r'rt[\\s]+', '', token)\n",
    "        token = re.sub(r'\\d+', '', token)\n",
    "        token = re.sub(r'\\$', '', token)\n",
    "        token = re.sub(r'rt+', '', token)\n",
    "        token = re.sub(r'https?:?\\/\\/\\S+', '', token)\n",
    "        if tag.startswith('NN'):\n",
    "            position = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            position = 'v'\n",
    "        elif tag.startswith('RB'):\n",
    "            position = 'r'\n",
    "        elif tag.startswith('JJ'):\n",
    "            position = 'a'\n",
    "        else:\n",
    "            position = 'n'\n",
    "\n",
    "        clean_list.append(lemmatizer.lemmatize(token, pos = position))\n",
    "        clean_list = [i for i in clean_list if i not in stop_words and len(i) > 0 and i != ':']\n",
    "\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "['top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "clean_positive = list(map(clean_tokens, positive_tokens))\n",
    "clean_negative = list(map(clean_tokens, negative_tokens))\n",
    "print(positive_tokens[0])\n",
    "print(clean_positive[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(tokens, status):\n",
    "    featureset = [(tweet, status) for tweet in tokens]\n",
    "    return featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['wanna', 'change', 'avi', 'usanele', ':('], 'Negative'),\n",
       " (['puppy', 'broke', 'foot', ':('], 'Negative'),\n",
       " ([\"where's\", 'jaebum', 'baby', 'picture', ':(', '('], 'Negative'),\n",
       " (['mr', 'ahmad', 'maslan', 'cook', ':('], 'Negative'),\n",
       " (['hull', 'suppoer', 'expect', 'misserable', 'week', ':-('], 'Negative')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_featureset = data_prepare(clean_positive, 'Positive')\n",
    "negative_featureset = data_prepare(clean_negative, 'Negative')\n",
    "\n",
    "featureset = positive_featureset + negative_featureset\n",
    "featureset[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['top', 'engage', 'member', 'community', 'week', ':)']\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "labels = []\n",
    "\n",
    "for x in featureset:\n",
    "    features.append(x[0])\n",
    "    labels.append(x[1])\n",
    "\n",
    "print(features[0])\n",
    "print(labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1, 2), sublinear_tf = True, max_features = 3000, preprocessor = ' '.join)\n",
    "vectorized_features = vectorizer.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'able', 'able see', 'abroad', 'absolute', 'absolutely', 'abt', 'acc', 'access', 'account', 'across', 'act', 'act like', 'active', 'actually']\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "vocabulary = vectorizer.get_feature_names()\n",
    "print(vocabulary[:15])\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_features, labels, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators = 200)\n",
    "classifier = text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Positive', 'Negative', ..., 'Positive', 'Negative',\n",
       "       'Negative'], dtype='<U8')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.70      0.82      0.76       730\n",
      "    Positive       0.80      0.67      0.73       770\n",
      "\n",
      "    accuracy                           0.74      1500\n",
      "   macro avg       0.75      0.75      0.74      1500\n",
      "weighted avg       0.75      0.74      0.74      1500\n",
      "\n",
      "0.7446666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@KirkKus', ':', 'Indirect', 'cost', 'of', 'the', 'UK', 'being', 'in', 'the', 'EU', 'is', 'estimated', 'to', 'be', 'costing', 'Britain', '£', '170', 'billion', 'per', 'year', '!', '#BetterOffOut', '#UKIP']\n",
      "['indirect', 'cost', 'uk', 'eu', 'estimate', 'cost', 'britain', 'billion', 'per', 'year']\n"
     ]
    }
   ],
   "source": [
    "test_tweets = twitter_samples.tokenized('tweets.20150430-223406.json')\n",
    "clean_test_tweets = list(map(clean_tokens, test_tweets))\n",
    "\n",
    "print(test_tweets[0])\n",
    "print(clean_test_tweets[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[indirect, cost, uk, eu, estimate, cost, brita...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[video, sturgeon, post-election, deal]</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[economy, grow, time, faster, day, david, came...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ukip, east, lothian, candidate, look, still, ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ukip's, housing, spokesman, rake, k, housing,...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[make, sure, tune, tonight, bbc]</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ed, milliband, embarrassment, would, want, re...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ft, back, tory, unrelated, note, here's, phot...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[“, ed, miliband, prove, tonight, he's, job, ”...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[lolz, trickle, wealth, never, trickle, past, ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets Sentiment\n",
       "0  [indirect, cost, uk, eu, estimate, cost, brita...  Positive\n",
       "1             [video, sturgeon, post-election, deal]  Negative\n",
       "2  [economy, grow, time, faster, day, david, came...  Negative\n",
       "3  [ukip, east, lothian, candidate, look, still, ...  Negative\n",
       "4  [ukip's, housing, spokesman, rake, k, housing,...  Negative\n",
       "5                   [make, sure, tune, tonight, bbc]  Positive\n",
       "6  [ed, milliband, embarrassment, would, want, re...  Negative\n",
       "7  [ft, back, tory, unrelated, note, here's, phot...  Positive\n",
       "8  [“, ed, miliband, prove, tonight, he's, job, ”...  Negative\n",
       "9  [lolz, trickle, wealth, never, trickle, past, ...  Negative"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Tweets'] = clean_test_tweets\n",
    "df['Sentiment'] = classifier.predict(vectorizer.transform(list(clean_test_tweets)))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
